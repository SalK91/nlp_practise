{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\salmank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\salmank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Gutenberg corpus if not already downloaded\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Gutenberg corpus\n",
    "gutenberg_corpus = gutenberg.sents()\n",
    "\n",
    "# Preprocess data \n",
    "Pre_process = True\n",
    "\n",
    "if Pre_process == True:\n",
    "    # Convert to lowercase\n",
    "    gutenberg_corpus = [[word.lower() for word in sentence] for sentence in gutenberg_corpus]\n",
    "    gutenberg_corpus\n",
    "\n",
    "    # Remove punctuations\n",
    "    punctuations = ['.', ',', '!', '?', ';', ':', '(', ')', '[', ']', '{', '}', \"'\",'\"', '``', \"''\", '--', '-', '—', '‘', '’', '“', '”', '.\"','!\"', '?\"',';\"','--\"','—\"','‘\"','’\"','“\"']\n",
    "    gutenberg_corpus = [[word for word in sentence if word not in punctuations] for sentence in gutenberg_corpus]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    gutenberg_corpus = [[word for word in sentence if word not in stop_words] for sentence in gutenberg_corpus]\n",
    "\n",
    "# Words in the corpus\n",
    "gutenberg_words = [word for sentence in gutenberg_corpus for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first 1000 sentences for training\n",
    "gutenberg_corpus_train = gutenberg_corpus[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 10000\n",
      "The number of tokens is 1101196\n",
      "The average number of tokens per sentence is 110\n",
      "The number of unique tokens are 42144\n"
     ]
    }
   ],
   "source": [
    "sents = gutenberg_corpus_train\n",
    "print(\"The number of sentences is\", len(sents)) \n",
    "\n",
    "words = gutenberg_words\n",
    "print(\"The number of tokens is\", len(words)) \n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\",average_tokens) \n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in gutenberg_corpus_train:\n",
    "    sequence = sentence \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)    \n",
    "    tokenized_text.append(sequence)\n",
    "    bigram.extend(list(ngrams(sequence, 2)))  \n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigrams:  [(('mr', 'knightley'), 277), ((',\"', 'said'), 270), (('mrs', 'weston'), 249), (('mr', 'elton'), 214), (('miss', 'woodhouse'), 173)]\n",
      "\n",
      "Most common trigrams:  [(('mr', 'frank', 'churchill'), 49), ((',\"', 'said', 'emma'), 46), ((',\"', 'said', 'mr'), 31), (('mr', 'john', 'knightley'), 29), (('dear', 'miss', 'woodhouse'), 24)]\n",
      "\n",
      "Most common fourgrams:  [((',\"', 'said', 'mr', 'knightley'), 18), ((',\"', 'said', 'mrs', 'weston'), 11), ((',\"', 'said', 'mr', 'woodhouse'), 7), ((',\"', 'said', 'frank', 'churchill'), 7), ((',\"', 'said', 'emma', 'smiling'), 5)]\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution of all the n-grams in the corpus\n",
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
    "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here. \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])             #add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('mr',), 0.011707644937737854], [('could',), 0.010190828254570886], [('would',), 0.009600450503637635], [('mrs',), 0.007911061862505564], [('emma',), 0.007874730923986594], [('must',), 0.00653048619878473], [('.--',), 0.0062398386906329754], [('miss',), 0.006103597671186841], [('much',), 0.005713040082107921], [('one',), 0.0055495508587725595]]\n",
      "\n",
      "Most common bigrams:  [[('mr', 'knightley'), 0.0016968394838678175], [(',\"', 'said'), 0.001654113309813592], [('mrs', 'weston'), 0.001525934787650915], [('mr', 'elton'), 0.0013123039173797868], [('miss', 'woodhouse'), 0.0010620506122050368], [('mr', 'weston'), 0.0009949094815483966], [('frank', 'churchill'), 0.0009277683508917563], [('mrs', 'elton'), 0.0008728346985363234], [('mr', 'woodhouse'), 0.0008117973070302868], [('captain', 'wentworth'), 0.0008056935678796831]]\n",
      "\n",
      "Most common trigrams:  [[('mr', 'frank', 'churchill'), 0.00030754090294009105], [(',\"', 'said', 'emma'), 0.00028908844876368556], [(',\"', 'said', 'mr'), 0.00019682617788165826], [('mr', 'john', 'knightley'), 0.0001845245417640546], [('dear', 'miss', 'woodhouse'), 0.00015377045147004553], [('said', 'mr', 'knightley'), 0.00012916717923483823], [(',\"', 'said', 'mrs'), 0.00012301636117603641], [('mr', 'mrs', 'weston'), 0.00011071472505843277], [('well', ',\"', 'said'), 9.841308894082913e-05], [('mr', 'mrs', 'musgrove'), 9.841308894082913e-05]]\n",
      "\n",
      "Most common fourgrams:  [[(',\"', 'said', 'mr', 'knightley'), 0.00012879522237511947], [(',\"', 'said', 'mrs', 'weston'), 8.134435097375967e-05], [(',\"', 'said', 'mr', 'woodhouse'), 5.422956731583978e-05], [(',\"', 'said', 'frank', 'churchill'), 5.422956731583978e-05], [('mr', 'mrs', 'john', 'knightley'), 4.0672175486879835e-05], [(',\"', 'said', 'emma', 'smiling'), 4.0672175486879835e-05], [(',\"', 'said', 'lady', 'russell'), 4.0672175486879835e-05], [(',\"', 'replied', 'mr', 'knightley'), 4.0672175486879835e-05], [('mr', 'frank', 'churchill', 'miss'), 4.0672175486879835e-05], [(',\"', 'cried', 'mr', 'weston'), 3.389347957239986e-05]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
    "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:  {1: ('knightley',), 2: ('mr', 'knightley'), 3: ('said', 'mr', 'knightley')} \n",
      "String 2:  {1: ('not',), 2: ('to', 'not'), 3: ('said', 'to', 'not')}\n"
     ]
    }
   ],
   "source": [
    "str1 = 'said mr knightley'\n",
    "str2 = 'he said to not'\n",
    "\n",
    "token_1 = nltk.word_tokenize(str1)\n",
    "token_2 = nltk.word_tokenize(str2)\n",
    "ngram_1 = {1:[], 2:[], 3:[]}   #to store the n-grams formed  \n",
    "ngram_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
    "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
    "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('knightley',)\n",
      "('mr', 'knightley')\n",
      "('said', 'mr', 'knightley')\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_1 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    print(ngram_1[i+1])\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_1[i+1]:      \n",
    "#to find predictions based on highest probability of n-grams  \n",
    "                 \n",
    "            count +=1\n",
    "            pred_1[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_1[i+1].append(\"NOT FOUND\")           \n",
    "#if no word prediction is found, replace with NOT FOUND\n",
    "            count +=1\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_2[i+1]:\n",
    "            count +=1\n",
    "            pred_2[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_2[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
      "\n",
      "String 1 - after that alice said the-\n",
      "\n",
      "Bigram model predictions: ['could', '.--', '!--', 'would', 'must']\n",
      "Trigram model predictions: ['would', 'could', 'mr', 'harriet', 'marrying']\n",
      "Fourgram model predictions: ['presently', 'warmly', 'feelingly', 'smile', 'nearly']\n",
      "\n",
      "String 2 - alice felt so desperate that she was-\n",
      "\n",
      "Bigram model predictions: ['\\x00', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "Trigram model predictions: ['\\x00', '\\x00', '\\x00', '\\x00', '\\x00']\n",
      "Fourgram model predictions: ['\\x00', '\\x00', '\\x00', '\\x00', '\\x00']\n"
     ]
    }
   ],
   "source": [
    "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
    "print(\"String 1 - after that alice said the-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
    "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
